{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST for Tensorflow-lite\n",
    "----\n",
    "\n",
    "This notebook is part of this [post](https://www.stupid-projects.com/machine-learning-on-embedded-part-3) which is part a series of post about using ML and NN in embedded MCUs. The first post of the series is [here](https://www.stupid-projects.com/machine-learning-on-embedded-part-1)\n",
    "\n",
    "This notebook is just a port of [this](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/5.1-introduction-to-convnets.ipynb) notebook from Keras to TF.\n",
    "\n",
    "This notebook is meant to be used to train the MNIST NN and then export the model to TF-Lite for microcontrollers and uploaded to a stm32f746. Later there's a cell in the notebook that you can hand-draw a number on a window and then evaluate the model on both the notebook and the stm32f746 by running the inference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create the model\n",
    "\n",
    "As it's mentioned before, this is just a port from Keras to TF of [this](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/5.1-introduction-to-convnets.ipynb) notebook. For the model training we're going to use `convnets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version 1.14.0\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"TensorFlow version {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels),(test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert train and test data\n",
    "Normally when the dataset is loaded the shape is (x, 28, 28). For convnets you need to reshape the data to (x, 28, 28, y), where `x` is the number of images per set and `y` in this case is the number of colors. Normally, of RGB it should be 3, but since the images are grayscale then it's 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'numpy.ndarray'>\n",
      "Dataset shape: (60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data type:\", type(train_images))\n",
    "print(\"Dataset shape:\", (train_images.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: 60000\n",
      "Possible values: [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels:\", len(train_labels))\n",
    "print(\"Possible values:\", np.unique(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print a digit from the dataset\n",
    "Now we just print a digit from the dataset in order to see how it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_img(img):\n",
    "    img = np.array(img, dtype='float')\n",
    "    pixels = img.reshape((28, 28))\n",
    "    plt.figure()\n",
    "    plt.imshow(pixels, cmap='gray')\n",
    "    plt.colorbar()\n",
    "    plt.grid(False)\n",
    "    plt.xlabel(\"Classification label: {}\".format(train_labels[0]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_img(train_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation=tf.nn.relu, input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation=tf.nn.relu),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 9s 157us/sample - loss: 0.1843 - acc: 0.9423\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 9s 156us/sample - loss: 0.0464 - acc: 0.9856\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 9s 156us/sample - loss: 0.0318 - acc: 0.9905\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 9s 157us/sample - loss: 0.0240 - acc: 0.9925\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 9s 157us/sample - loss: 0.0183 - acc: 0.9945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fca98763080>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 69us/sample - loss: 0.0339 - acc: 0.9907\n",
      "Test accuracy: 0.9907\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10000/10000 [==============================] - 1s 68us/sample - loss: 0.0339 - acc: 0.9907\n",
      "Restored model, accuracy: 99.07%\n",
      "Restored model, loss: 0.03389284939114805\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "loss, acc = model.evaluate(test_images, test_labels)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))\n",
    "print(\"Restored model, loss: {}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert the model to tflite\n",
    "\n",
    "Now we need to export the model and save it in a `h5` file. Then we use the `TFLiteConverter` to convert the model to the flatbuffer tflite format.\n",
    "\n",
    "Normally, we should use quantization on the model as it's explained [here](https://www.tensorflow.org/lite/microcontrollers/build_convert#quantization), but for some reason in the current version I'm using (1.14) that doesn't work and when the model is loaded on the stm32f746, then I get this error:\n",
    "\n",
    "```\n",
    "Only float32, int16, int32, int64, uint8, bool, complex64 supported currently\n",
    "```\n",
    "\n",
    "This error comes from the `source/libs/tensorflow/lite/experimental/micro/simple_tensor_allocator.cc` file and the reason is that when the model is converted with `TFLiteConverter`, then the output is set to `kTfLiteInt8`, which means signed integer and that is not yet supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Fetch argument <tf.Variable 'conv2d_3/kernel:0' shape=(3, 3, 1, 32) dtype=float32> cannot be interpreted as a Tensor. (Tensor Tensor(\"conv2d_3/kernel/Read/ReadVariableOp:0\", shape=(3, 3, 1, 32), dtype=float32) is not an element of this graph.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/nn-env/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    302\u001b[0m         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n\u001b[0;32m--> 303\u001b[0;31m             fetch, allow_tensor=True, allow_operation=True))\n\u001b[0m\u001b[1;32m    304\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nn-env/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3795\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3796\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nn-env/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3874\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3875\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tensor %s is not an element of this graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3876\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor Tensor(\"conv2d_3/kernel/Read/ReadVariableOp:0\", shape=(3, 3, 1, 32), dtype=float32) is not an element of this graph.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-cdb6ed04d54f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mnist_tflite.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/nn-env/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format)\u001b[0m\n\u001b[1;32m   1209\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m     \"\"\"\n\u001b[0;32m-> 1211\u001b[0;31m     \u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nn-env/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format)\u001b[0m\n\u001b[1;32m    111\u001b[0m           'or using `save_weights`.')\n\u001b[1;32m    112\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[0;32m--> 113\u001b[0;31m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nn-env/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mmodel_weights_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mmodel_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weights_group\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minclude_optimizer\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nn-env/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_weights_to_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_legacy_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m     \u001b[0mweight_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m     \u001b[0mweight_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0msave_attributes_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weight_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nn-env/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m   3008\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot get value inside Tensorflow graph function.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3009\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3010\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3011\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3012\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nn-env/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nn-env/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1156\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1158\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nn-env/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    472\u001b[0m     \"\"\"\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nn-env/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_ListFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_DictFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nn-env/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches)\u001b[0m\n\u001b[1;32m    371\u001b[0m     \"\"\"\n\u001b[1;32m    372\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nn-env/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    371\u001b[0m     \"\"\"\n\u001b[1;32m    372\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nn-env/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m     \u001b[0;31m# Did not find anything.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' % (fetch,\n",
      "\u001b[0;32m~/miniconda3/envs/nn-env/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    308\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n\u001b[0;32m--> 310\u001b[0;31m                          'Tensor. (%s)' % (fetch, str(e)))\n\u001b[0m\u001b[1;32m    311\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n",
      "\u001b[0;31mValueError\u001b[0m: Fetch argument <tf.Variable 'conv2d_3/kernel:0' shape=(3, 3, 1, 32) dtype=float32> cannot be interpreted as a Tensor. (Tensor Tensor(\"conv2d_3/kernel/Read/ReadVariableOp:0\", shape=(3, 3, 1, 32), dtype=float32) is not an element of this graph.)"
     ]
    }
   ],
   "source": [
    "model.save('mnist-tflite.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: If you want to add post-quantization during conversion (which doesn't work yet), then you need to uncomment the line in the next code. Finally, the output of the next command is the size of the flatbuffer model in bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the converted flatbuffer is: 375724 bytes\n"
     ]
    }
   ],
   "source": [
    "tflite_mnist_model = 'mnist.tflite'\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model_file('mnist_keras.h5')\n",
    "# converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "flatbuffer_size = open(tflite_mnist_model, \"wb\").write(tflite_model)\n",
    "\n",
    "print('The size of the converted flatbuffer is: %d bytes' % flatbuffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a header file from the tflite model\n",
    "Now that you have your tflite flatbuffer you can convert it to a header file\n",
    "in order to add it to your C++ code you need to run this command in bash in\n",
    "the `jupyter _notebook` folder.\n",
    "\n",
    "```sh\n",
    "xxd -i jupyter_notebook/mnist-quant.tflite > source/src/inc/model_data.h\n",
    "```\n",
    "\n",
    "#### Note:\n",
    "In the `source/src/inc/model_data.h` you need to change this line:\n",
    "```cpp\n",
    "unsigned char jupyter_notebook_mnist_tflite[] = {\n",
    "```\n",
    "to this:\n",
    "```cpp\n",
    "const unsigned char jupyter_notebook_mnist_tflite[] = {\n",
    "```\n",
    "Otherwise it won't fit in the RAM and you'll get this error:\n",
    "```sh\n",
    "stm32f7-mnist-tflite.elf section `.bss' will not fit in region `RAM'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load model and interpreter\n",
    "\n",
    "To evaluate the hand-written digit in the notebook then you need to create an interpreter and then feed the image (or array) in the input. You can create that here and use it later in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Input details ==\n",
      "name: conv2d_input\n",
      "shape: [ 1 28 28  1]\n",
      "type: <class 'numpy.float32'>\n",
      "\n",
      "== Output details ==\n",
      "name: dense_1/Softmax\n",
      "shape: [ 1 10]\n",
      "type: <class 'numpy.float32'>\n",
      "\n",
      "DUMP INPUT\n",
      "{'name': 'conv2d_input', 'index': 9, 'shape': array([ 1, 28, 28,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n",
      "\n",
      "DUMP OUTPUT\n",
      "{'name': 'dense_1/Softmax', 'index': 15, 'shape': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=tflite_mnist_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(\"name:\", interpreter.get_input_details()[0]['name'])\n",
    "print(\"shape:\", interpreter.get_input_details()[0]['shape'])\n",
    "print(\"type:\", interpreter.get_input_details()[0]['dtype'])\n",
    "\n",
    "print(\"\\n== Output details ==\")\n",
    "print(\"name:\", interpreter.get_output_details()[0]['name'])\n",
    "print(\"shape:\", interpreter.get_output_details()[0]['shape'])\n",
    "print(\"type:\", interpreter.get_output_details()[0]['dtype'])\n",
    "\n",
    "print(\"\\nDUMP INPUT\")\n",
    "print(interpreter.get_input_details()[0])\n",
    "print(\"\\nDUMP OUTPUT\")\n",
    "print(interpreter.get_output_details()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evalutate the model\n",
    "\n",
    "In order to make it more interesting, I've wrote a small python that you can draw a digit with your mouse and then ran the prediction function to evaluate the result. For this purpose I'm using tkinter and the PIL library. Therefore you need to install them in your environment.\n",
    "\n",
    "For ubuntu (I'm using conda):\n",
    "```sh\n",
    "sudo apt install python3-tk\n",
    "conda install Pillow\n",
    "```\n",
    "\n",
    "#### How to use:\n",
    "Run the following two cells and this window will show up.\n",
    "\n",
    "![Image](./MnistDigitDraw/digit_draw_1.png)\n",
    "\n",
    "In the left window you can draw any digit with your mouse by clicking in the white area. Just be sure that you don't draw that too fast because then you get dotted lines. Then you can either press one of the following buttons:\n",
    "* `Clear`: clears the input drawing area\n",
    "* `Export`: Converts the draw digit to the MNIST input format and then exports the digit to a file called `digit.txt`. You can use this file in this notebook and evaluate the result.\n",
    "* `Inference`: Converts the digit to the MNIST input format and sends the data to the MCU via the serial port. Then the MCU runs the prediction and returns an array with the output values and the time that spend for the calculation.\n",
    "\n",
    "This is an example (I'm right-handed but I'm the mouse with my left hand, this is why it seems so ugly, lol).\n",
    "\n",
    "![Image](./MnistDigitDraw/digit_draw_2.png)\n",
    "\n",
    "\n",
    "Anyway, try yourself by running the next two cells.\n",
    "\n",
    "> Warning: If you proceed with the export function and local evaluation, then you need first to terminate the tkinter window, because the notebook is not able to run 2 cells at the same time. Therefore, if the window thread is running then no other cell can be run.\n",
    "\n",
    "> Note: Be carefull that if you do any changes in any python class or script that is already loaded from the jupyter notebook kernel, then you need to restart the kernel (File menu: Kernel-> Restart). Otherwise the previous loaded class will be used!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from tkinter import *\n",
    "from MnistDigitDraw.MnistDigitDraw import MnistDigitDraw\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the followinf cell how many times you like in order to draw a new digit every time. When you do, then press the `Inference` button, then the `Export` and then close the window to proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28, 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open the TK window to draw the digit and save it in a file\n",
    "root = Tk()\n",
    "root.title(\"MNIST digit draw\")\n",
    "d = MnistDigitDraw(root, 250, 250)\n",
    "d.start()\n",
    "root.mainloop()\n",
    "\n",
    "# Load digit from the file that created above\n",
    "digit = np.loadtxt('digit.txt')\n",
    "# Reshape\n",
    "loaded_digit = digit.reshape(28,28)\n",
    "loaded_digit = np.expand_dims(loaded_digit, axis=0)\n",
    "loaded_digit = np.expand_dims(loaded_digit, axis=3)\n",
    "loaded_digit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load the digit that you drew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_img(loaded_digit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate hand-written digit on the notebook\n",
    "\n",
    "In case you wnt to test your hand-written digit here then run the following cells. The next cells will load the `digit.txt` file which was exported in the previous step.\n",
    "\n",
    "The imported digit share is (768,). If you just open the file in a text editor (or cat the file) you'll see that each pixel value is one line. Therefore, after loading the digit then you need to convert from `(768,)` to `(1, 28, 28, 1)`, which is the format that the model prediction function support. To do that you need first to reshape the array to `(28, 28)` and then add two additional dimension in the tensor, one dimension in the beginning and one in the end. Those don't need to have a value, the first dimension is used as an index and the last as the prediction result, but it this case we don't care for any of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction results: [[7.62192826e-12 1.12496224e-07 5.35912079e-07 2.98589924e-07\n",
      "  7.58220597e-10 7.78454912e-10 9.02060612e-15 9.99999046e-01\n",
      "  2.11351869e-09 2.85951973e-09]]\n",
      "Predicted value: 7\n"
     ]
    }
   ],
   "source": [
    "loaded_digit = loaded_digit.astype('float32')\n",
    "input_details = interpreter.get_input_details()\n",
    "interpreter.set_tensor(input_details[0]['index'], loaded_digit)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "output_details = interpreter.get_output_details()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(\"Prediction results:\", output_data)\n",
    "print(\"Predicted value:\", np.argmax(output_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above result the one which has the larger value is the predicted digit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate on the STM32F746\n",
    "\n",
    "Now you can convert the digit and upload it to the stm32 for evaluation in the model that is running in the embedded board. See in section 10, how to connect the board to your PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the custom flatbuffer communication module for the STM32\n",
    "from FbComm.FbComm import FbComm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Check connection\n",
    "To check the connection between your workstation and the stm32f743 you can run the following command, which will retrieve the version and the CPU frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comm initialized\n",
      "Requesting stats...\n",
      "Receiving stats...\n",
      "Command: 0\n",
      "Version: 100\n",
      "Freq: 216000000\n"
     ]
    }
   ],
   "source": [
    "# Select the proper com port for your system\n",
    "serial = FbComm(uart = '/dev/ttyUSB0')\n",
    "serial.reqStats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Test hand-draw digit\n",
    "\n",
    "Now you can test the digit you've drawn on the STM32 by running the following cell. Then you can compare the result you got from step #8\n",
    "\n",
    "> Note: This script might take a few seconds to return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comm initialized\n",
      "Num of elements: 784\n",
      "Sending image data\n",
      "Receive results...\n",
      "Command: 2\n",
      "Execution time: 2991 msec\n",
      "Out[9]: 0.000000\n",
      "Out[8]: 0.000000\n",
      "Out[7]: 0.999986\n",
      "Out[6]: 0.000000\n",
      "Out[5]: 0.000000\n",
      "Out[4]: 0.000000\n",
      "Out[3]: 0.000000\n",
      "Out[2]: 0.000007\n",
      "Out[1]: 0.000007\n",
      "Out[0]: 0.000000\n"
     ]
    }
   ],
   "source": [
    "serial = FbComm(uart = '/dev/ttyUSB0')\n",
    "serial.reqInference(digit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export hand-draw digit for use in the source code\n",
    "\n",
    "For testing and development you can export the digit you've drawn in a header file and then compile the code and upload it to the stm32f7. When you do that, then in order to save RAM you need to convert the array to `const`. The following script does that for you, though.\n",
    "\n",
    "> Note: Only run the next cell if you want to override the default digit in the flash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes = digit.tobytes\n",
    "np.save(\"bytes.txt\", bytes)\n",
    "f= open(\"../source/src/inc/digit.h\",\"w+\")\n",
    "f.write(\"const float digit[] = { \\n\")\n",
    "for d in digit:\n",
    "    f.write(str(d))\n",
    "    f.write(',')\n",
    "f.write('\\n};')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Build the code and compile\n",
    "\n",
    "Now on your console run these commands to build and flash the project on the stm32f746\n",
    "\n",
    "```sh\n",
    "CLEANBUILD=true USE_HAL_DRIVER=ON SRC=src ./build.sh\n",
    "./flash.sh\n",
    "```\n",
    "\n",
    "> Note: Before you build you need to set the toolchain in your `source/CMakeLists.txt` file. Open the file and uncomment this line:\n",
    "```cmake\n",
    "set(TOOLCHAIN_DIR \"/opt/toolchains/gcc-arm-none-eabi-8-2018-q4-major\")\n",
    "```\n",
    "Then insert the path of your toolchain. You can find the latest toolchain [here](https://developer.arm.com/tools-and-software/open-source-software/developer-tools/gnu-toolchain/gnu-rm/downloads)\n",
    "\n",
    "You can repeat hand-drawing and upload or flash procedure as many times you like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Testing the FbComm module\n",
    "\n",
    "The FbComm.py is a Python module that I've made in order to create a proxy between this notebook and the STM32F7 board in order to use the serial port to send/receive data. Because during development, testing and debuging is not optimal (and actually very slow) to test the communication and the parsing of the flatbuffers by compiliing and uploading code to the board; I've also written a small C++ tool that used TCP sockets and `emulates` the board.\n",
    "\n",
    "The `FbComm` class in the `FbComm.py` supports both a serial and a TCP connection. Therefore, you can use your PC to develop the C++ code for develop the serialization and de-serialization with flatbuffers. You can do that by using the `jupyter_notebook/FbComm/test_cpp_app/fb_comm_test.cpp` and `jupyter_notebook/FbComm/FbComm.py` with the TCP connection.\n",
    "\n",
    "For example, this is how to use `FbComm` with TCP:\n",
    "\n",
    "```py\n",
    "com = FbComm(ip='127.0.0.1', port='32001')\n",
    "com.reqStats()\n",
    "com.reqInference(digit)\n",
    "```\n",
    "\n",
    "And this is for the UART/Serial port:\n",
    "```py\n",
    "com = FbComm(uart='/dev/ttyUSB0')\n",
    "com.reqStats()\n",
    "com.reqInference(digit)\n",
    "```\n",
    "\n",
    "If you run this outside from the notepad then you need first to save a digit from the notepad and then load the digit from the python code. Like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will save the digit to 'digit.npy' file.\n",
    "np.save('digit', digit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now from your script you can load the digit and use FbComm like this:\n",
    "```py\n",
    "digit = np.load('digit.npy')\n",
    "com = FbComm(ip='127.0.0.1', port='32001')\n",
    "com.reqStats()\n",
    "com.reqInference(digit)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Connecting the board to your PC\n",
    "\n",
    "The STM32F7 firmware supports two UART ports (both @ 115200 bps). Those two have different usage:\n",
    "\n",
    "#### 13.1 `USART6`\n",
    "Debug port that accepts ASCII commands and also prints out debugging messages. The handler for this port is in `main.cpp` in the `dbg_uart_parser()` function and the supported commands for this port are:\n",
    "\n",
    "```CMD=<command id>\\n```\n",
    "where `\\n`, is a newline character and `<command id>` is a number with the following meaning:\n",
    "\n",
    "* `1`: Prints the NN model view. Example with print-out:\n",
    "\n",
    "Command:\n",
    "```sh\n",
    "CMD=1\n",
    "```\n",
    "Response:\n",
    "```sh\n",
    "Received: CMD=1\n",
    "\n",
    "Model input:\n",
    "dims->size: 4\n",
    "dims->data[0]: 1\n",
    "dims->data[1]: 28\n",
    "dims->data[2]: 28\n",
    "dims->data[3]: 1\n",
    "input->type: 1\n",
    "\n",
    "Model output:\n",
    "dims->size: 2\n",
    "dims->data[0]: 1\n",
    "dims->data[1]: 10\n",
    "\n",
    "Testing mode: 1\n",
    "```\n",
    "* `2`: Runs the inference by using the hard-coded digit in `source/src/inc/digit.h` and prints stats with how much time each layer takes. Read #10 on how to change this.\n",
    "\n",
    "Command:\n",
    "```sh\n",
    "CMD=1\n",
    "```\n",
    "Response:\n",
    "```sh\n",
    "Received: CMD=2\n",
    "\n",
    "Running inference...\n",
    "DEPTHWISE_CONV_2D: 243 msec\n",
    "\n",
    "MAX_POOL_2D: 23 msec\n",
    "\n",
    "CONV_2D: 2347 msec\n",
    "\n",
    "MAX_POOL_2D: 7 msec\n",
    "\n",
    "CONV_2D: 348 msec\n",
    "\n",
    "FULLY_CONNECTED: 6 msec\n",
    "\n",
    "FULLY_CONNECTED: 0 msec\n",
    "\n",
    "SOFTMAX: 0 msec\n",
    "\n",
    "Done in 2990 msec...\n",
    "Out[0]: 0.000000\n",
    "Out[1]: 0.000000\n",
    "Out[2]: 0.000000\n",
    "Out[3]: 0.000000\n",
    "Out[4]: 0.000000\n",
    "Out[5]: 1.000000\n",
    "Out[6]: 0.000000\n",
    "Out[7]: 0.000000\n",
    "Out[8]: 0.000000\n",
    "Out[9]: 0.000000\n",
    "Testing mode: 2\n",
    "```\n",
    "\n",
    "> Note: You can expand the supported commands in `main.cpp::dbg_uart_parser()`.\n",
    "\n",
    "#### 13.2 `UART7`\n",
    "\n",
    "This is the flatbuffer uart port which is used for the communication between this notepad and the STM32F7.\n",
    "\n",
    "#### 13.3 Connections\n",
    "\n",
    "The [STM32F746G-DISCO](https://www.st.com/en/evaluation-tools/32f746gdiscovery.html#overview) has an Arduino UNO-like connector. This connector has access to two UART ports (USART6 & UART7) and there you can connect a TTL-level USB-to-serial module (the pins seem to be 5V tolerant).\n",
    "\n",
    "PORT | Connector PIN\n",
    "-|-\n",
    "USART6-RX | D0\n",
    "USART6-TX | D1\n",
    "UART7-TX | A4\n",
    "UART7-RX | A5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
